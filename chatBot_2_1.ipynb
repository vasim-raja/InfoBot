{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NTaYYPSkItNm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "pMmCiMOqJLNE",
        "outputId": "b6763ffb-c313-46f0-e5ff-d3fae884037d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIn transfer learning, the knowledge of an already trained machine learning model is applied to a different but related problem. For example, if you trained a simple classifier to predict whether an image contains a backpack, you could use the knowledge that the model gained during its training to recognize other objects like sunglasses.\\n\\nWith transfer learning, we basically try to exploit what has been learned in one task to improve generalization in another. We transfer the weights that a network has learned at “task A” to a new “task B.”\\n\\nThe general idea is to use the knowledge a model has learned from a task with a lot of available labeled training data in a new task that doesn\\'t have much data. Instead of starting the learning process from scratch, we start with patterns learned from solving a related task.\\n\\nTransfer learning is mostly used in computer vision and natural language processing tasks like sentiment analysis due to the huge amount of computational power required.\\n\\nTransfer learning isn’t really a machine learning technique, but can be seen as a “design methodology” within the field, for example, active learning. It is also not an exclusive part or study-area of machine learning. Nevertheless, it has become quite popular in combination with neural networks that require huge amounts of data and computational power.\\n\\n \\n\\nTransfer Learning Works In computer vision, for example, neural networks usually try to detect edges in the earlier layers, shapes in the middle layer and some task-specific features in the later layers. In transfer learning, the early and middle layers are used and we only retrain the latter layers. It helps leverage the labeled data of the task it was initially trained on.\\n\\nLet’s go back to the example of a model trained for recognizing a backpack on an image, which will be used to identify sunglasses. In the earlier layers, the model has learned to recognize objects, because of that we will only retrain the latter layers so it will learn what separates sunglasses from other objects.\\n\\nclassifiers transfer learning\\n\\nIn transfer learning, we try to transfer as much knowledge as possible from the previous task the model was trained on to the new task at hand. This knowledge can be in various forms depending on the problem and the data. For example, it could be how models are composed, which allows us to more easily identify novel objects.\\n\\n\\nWhy Use Transfer Learning\\nTransfer learning has several uses, but the main advantages are saving training time, better performance of neural networks (in most cases), and not needing a lot of data. \\n\\nUsually, a lot of data is needed to train a neural network from scratch but access to that data isn\\'t always available — this is where transfer learning comes in handy. With transfer learning a solid machine learning model can be built with comparatively little training data because the model is already pre-trained. This is especially valuable in natural language processing because mostly expert knowledge is required to create large labeled datasets. Additionally, training time is reduced because it can sometimes take days or even weeks to train a deep neural network from scratch on a complex task.\\n\\nAccording to DeepMind CEO Demis Hassabis, transfer learning is also one of the most promising techniques that could lead to artificial general intelligence (AGI) someday:\\n\\nAGI transfer learning\\n\\n \\n\\nWhen to Use Transfer Learning\\nAs is always the case in machine learning, it is hard to form rules that are generally applicable, but here are some guidelines on when transfer learning might be used:\\n\\nThere isn\\'t enough labeled training data to train your network from scratch.\\nThere already exists a network that is pre-trained on a similar task, which is usually trained on massive amounts of data.\\nWhen task 1 and task 2 have the same input.\\nIf the original model was trained using TensorFlow, you can simply restore it and retrain some layers for your task. Keep in mind, however, that transfer learning only works if the features learned from the first task are general, meaning they can be useful for another related task as well. Also, the input of the model needs to have the same size as it was initially trained with. If you don’t have that, add a pre-processing step to resize your input to the needed size.\\n\\n \\n\\nApproaches to Transfer Learning\\n1. TRAINING A MODEL TO REUSE IT\\nImagine you want to solve task A but don’t have enough data to train a deep neural network. One way around this is to find a related task B with an abundance of data. Train the deep neural network on task B and use the model as a starting point for solving task A. Whether you\\'ll need to use the whole model or only a few layers depends heavily on the problem you\\'re trying to solve.\\n\\nIf you have the same input in both tasks, possibly reusing the model and making predictions for your new input is an option. Alternatively, changing and retraining different task-specific layers and the output layer is a method to explore.\\n\\n2. USING A PRE-TRAINED MODEL\\nThe second approach is to use an already pre-trained model. There are a lot of these models out there, so make sure to do a little research. How many layers to reuse and how many to retrain depends on the problem. \\n\\nKeras, for example, provides nine pre-trained models that can be used for transfer learning, prediction, feature extraction and fine-tuning. You can find these models, and also some brief tutorials on how to use them, here. There are also many research institutions that release trained models.\\n\\nThis type of transfer learning is most commonly used throughout deep learning.\\n\\n3. FEATURE EXTRACTION\\nAnother approach is to use deep learning to discover the best representation of your problem, which means finding the most important features. This approach is also known as representation learning, and can often result in a much better performance than can be obtained with hand-designed representation.\\n\\nfeature extraction transfer learning\\n\\nIn machine learning, features are usually manually hand-crafted by researchers and domain experts. Fortunately, deep learning can extract features automatically. Of course, this doesn\\'t mean feature engineering and domain knowledge isn’t important anymore — you still have to decide which features you put into your network. That said, neural networks have the ability to learn which features are really important and which ones aren’t. A representation learning algorithm can discover a good combination of features within a very short timeframe, even for complex tasks which would otherwise require a lot of human effort.\\n\\nThe learned representation can then be used for other problems as well. Simply use the first layers to spot the right representation of features, but don’t use the output of the network because it is too task-specific. Instead, feed data into your network and use one of the intermediate layers as the output layer. This layer can then be interpreted as a representation of the raw data.\\n\\nThis approach is mostly used in computer vision because it can reduce the size of your dataset, which decreases computation time and makes it more suitable for traditional algorithms, as well.\\n\\n \\n\\nPOPULAR PRE-TRAINED MODELS\\nThere are some pre-trained machine learning models out there that are quite popular. One of them is the Inception-v3 model, which was trained for the ImageNet “Large Visual Recognition Challenge.\" In this challenge, participants had to classify images into 1,000 classes like “zebra,\" “Dalmatian\" and “dishwasher.\"\\n\\nHere\\'s a very good tutorial from TensorFlow on how to retrain image classifiers.\\n\\nMicrosoft also offers some pre-trained models, available for both R and Python development, through the MicrosoftML R package and the Microsoftml Python package.\\n\\nOther quite popular models are ResNet and AlexNet. I also encourage a visit to pretrained.ml, a sortable and searchable compilation of pre-trained deep learning models complete with demos and code.\\n\\nBERT\\nBERT stands for Bidirectional Encoder Representations from Transformers. It is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of NLP tasks.\\n\\nThe BERT architecture builds on top of Transformer. We currently have two variants available:\\n\\nBERT Base: 12 layers (transformer blocks), 12 attention heads, and 110 million parameters\\nBERT Large: 24 layers (transformer blocks), 16 attention heads and, 340 million parameters\\n\\nBERT is basically an Encoder stack of transformer architecture. A transformer architecture is an encoder-decoder network that uses self-attention on the encoder side and attention on the decoder side. BERTBASE has 12 layers in the Encoder stack while BERTLARGE has 24 layers in the Encoder stack. These are more than the Transformer architecture described in the original paper (6 encoder layers). BERT architectures (BASE and LARGE) also have larger feedforward-networks (768 and 1024 hidden units respectively), and more attention heads (12 and 16 respectively) than the Transformer architecture suggested in the original paper. It contains 512 hidden units and 8 attention heads. BERTBASE contains 110M parameters while BERTLARGE has 340M parameters.\\n\\nGLUE: The General Language Understanding Evaluation task is a collection of different Natural Language Understanding tasks. These include MNLI (Multi-Genre Natural Language Inference), QQP(Quora Question Pairs), QNLI(Question Natural Language Inference), SST-2(The Stanford Sentiment Treebank), CoLA(Corpus of Linguistic Acceptability) etc. Both, BERTBASE and BERTLARGE outperforms previous models by a good margin (4.5% and 7% respectively). Below are the results of BERTBASE and BERTLARGE as compared to other models:\\n\\nBertConfig\\nclasstransformers.BertConfig(vocab_size=30522, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, hidden_act=\\'gelu\\', hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, max_position_embeddings=512, type_vocab_size=2, initializer_range=0.02, layer_norm_eps=1e-12, pad_token_id=0, gradient_checkpointing=False, **kwargs)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "f = open(\"//content/drive/MyDrive/my projects/chatbots/trnsfrlngBERTdata.txt\")\n",
        "corpus = f.read()\n",
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fki9jEBuJtoS",
        "outputId": "e0089c45-2e78-4e66-c1a3-598cd98c7a81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "corpus = corpus.lower()\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb8LgzuULgEK"
      },
      "source": [
        "TokeniZation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fHDjmhPGKTwt"
      },
      "outputs": [],
      "source": [
        "sentence_tokens = nltk.sent_tokenize(corpus)\n",
        "word_tokens = nltk.word_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KikD51jrKo_g",
        "outputId": "e8e95fdb-8832-4ee6-c122-1c34a4b85629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\nin transfer learning, the knowledge of an already trained machine learning model is applied to a different but related problem.', 'for example, if you trained a simple classifier to predict whether an image contains a backpack, you could use the knowledge that the model gained during its training to recognize other objects like sunglasses.', 'with transfer learning, we basically try to exploit what has been learned in one task to improve generalization in another.', \"we transfer the weights that a network has learned at “task a” to a new “task b.”\\n\\nthe general idea is to use the knowledge a model has learned from a task with a lot of available labeled training data in a new task that doesn't have much data.\"]\n",
            "['in', 'transfer', 'learning', ',']\n"
          ]
        }
      ],
      "source": [
        "from nltk.text import sent_tokenize\n",
        "print(sentence_tokens[:4])\n",
        "print(word_tokens[:4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TQmKIwwLmXy"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CNd7PV12Kx1C"
      },
      "outputs": [],
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def lemtokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "rempun = dict((ord(punct), None) for punct in string.punctuation)\n",
        "def lemnormalize(text):\n",
        "  return lemtokens(nltk.word_tokenize(text.lower().translate(rempun)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Jp1nAoHXNf6O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjsg8XDBOGik"
      },
      "source": [
        "Greeting Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LU1l4qGwOGLc"
      },
      "outputs": [],
      "source": [
        "greet_inputs = (\"hello\",\"hi\",\"hai\",\"whassup\",\"hey\",\"hey there\") \n",
        "greet_responses = (\"hello\",\"hi\",\"hai\",\"whassup\",\"hey\",\"hey there\",\"there you are\")\n",
        "\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in greet_inputs:\n",
        "      return random.choice(greet_responses)\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v4Je7laqPFx0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DYyfEMTPeEL"
      },
      "source": [
        "Brain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FYq6c0ktPe37"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sFgmgdBrPsSb"
      },
      "outputs": [],
      "source": [
        "def response(user_response):\n",
        "  robo1_resp = \"\"\n",
        "  tfidfvec = TfidfVectorizer(tokenizer= lemnormalize, stop_words=\"english\")\n",
        "  tfid = tfidfvec.fit_transform(sentence_tokens)\n",
        "  vals = cosine_similarity(tfid[-1],tfid)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfid = flat[-2]\n",
        "  if req_tfid == 0:\n",
        "    robo1_resp = robo1_resp + \" sorry, am not able to understand you\"\n",
        "    return robo1_resp\n",
        "  else:\n",
        "    robo1_resp = robo1_resp + sentence_tokens[idx]\n",
        "    return robo1_resp\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Z6bekqz6Ub_J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjnao7fzVUF6"
      },
      "source": [
        "ChatFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKTWEITBVTip",
        "outputId": "59ea2a1b-3b68-4243-9361-23ce9aa3ff51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hai, am a chatbot having Intelligence about transfer learning. you can ask me anything around transfer learning.to end my service you can type bye....\n",
            "hello\n",
            "bot:hi\n",
            "what is transfer learning\n",
            "bot:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classifiers transfer learning\n",
            "\n",
            "in transfer learning, we try to transfer as much knowledge as possible from the previous task the model was trained on to the new task at hand.\n",
            "what is bert\n",
            "bot:bert\n",
            "bert stands for bidirectional encoder representations from transformers.\n",
            "what is glue\n",
            "bot:glue: the general language understanding evaluation task is a collection of different natural language understanding tasks.\n",
            "tell me about bert architecture\n",
            "bot:the bert architecture builds on top of transformer.\n",
            "popular pre trained models\n",
            "bot:popular pre-trained models\n",
            "there are some pre-trained machine learning models out there that are quite popular.\n",
            "how feature extraction transfer learning\n",
            "bot:feature extraction transfer learning\n",
            "\n",
            "in machine learning, features are usually manually hand-crafted by researchers and domain experts.\n",
            "tell me about universe\n",
            "bot: sorry, am not able to understand you\n",
            "bye\n",
            "Bot: Good Bye.....\n"
          ]
        }
      ],
      "source": [
        "\n",
        "flag  = True\n",
        "print(\"Hai, am a chatbot having Intelligence about transfer learning. you can ask me anything around transfer learning.to end my service you c\\\n",
        "an type bye....\")\n",
        "while(flag == True):\n",
        "  user_response = input()\n",
        "  user_response = user_response.lower()\n",
        "  if (user_response != \"bye\"):\n",
        "    if(user_response == \"thankyou\" or user_response== \"thanks\"):\n",
        "      flag = False\n",
        "      print(\"Bot: You are Welcome....\")\n",
        "    else:\n",
        "        if(greet(user_response) != None):\n",
        "          print(\"bot:\"+ greet(user_response))\n",
        "        else:\n",
        "          sentence_tokens.append(user_response)\n",
        "          word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
        "          final_words = list(set(word_tokens))\n",
        "          print(\"bot:\",end= '')\n",
        "          print(response(user_response))\n",
        "          sentence_tokens.remove(user_response)\n",
        "  else:\n",
        "    flag = False\n",
        "    print(\"Bot: Good Bye.....\")\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "W4H6HJnnVnzN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}